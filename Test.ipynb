{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize Cerebras client\n",
    "api_key = os.environ.get(\"CEREBRAS_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY environment variable is not set.\")\n",
    "\n",
    "client = Cerebras(api_key=api_key)\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Function to interact with Cerebras API\n",
    "def get_cerebras_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama3.1-8b\",\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# Create a conversation chain\n",
    "def chat():\n",
    "    print(\"Welcome to the Cerebras Chat! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to memory\n",
    "        memory.chat_memory.add_user_message(user_input)\n",
    "\n",
    "        # Get the conversation history from memory\n",
    "        history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "        # Generate a prompt with conversation history\n",
    "        prompt = f\"{history}\\nUser: {user_input}\\nAI:\"\n",
    "\n",
    "        # Get response from Cerebras API\n",
    "        ai_response = get_cerebras_response(prompt)\n",
    "\n",
    "        # Add AI response to memory\n",
    "        memory.chat_memory.add_ai_message(ai_response)\n",
    "\n",
    "        # Print AI response\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Cerebras client\n",
    "api_key = os.environ.get(\"CEREBRAS_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY environment variable is not set.\")\n",
    "\n",
    "client = Cerebras(api_key=api_key)\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Function to interact with Cerebras API\n",
    "def get_cerebras_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"llama3.1-8b\",\n",
    "    )\n",
    "    # Debug: Print the entire response to inspect its structure\n",
    "    print(\"Response Object:\", response)\n",
    "    \n",
    "    # Adjust this based on the actual response structure\n",
    "    # Example: If response.choices[0].message.content is the correct path\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create a conversation chain\n",
    "def chat():\n",
    "    print(\"Welcome to the Cerebras Chat! Type '/exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to memory\n",
    "        memory.chat_memory.add_user_message(user_input)\n",
    "\n",
    "        # Get the conversation history from memory\n",
    "        history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "        # Generate a prompt with conversation history\n",
    "        prompt = f\"{history}\\nUser: {user_input}\\nAI:\"\n",
    "\n",
    "        # Get response from Cerebras API\n",
    "        ai_response = get_cerebras_response(prompt)\n",
    "\n",
    "        # Add AI response to memory\n",
    "        memory.chat_memory.add_ai_message(ai_response)\n",
    "\n",
    "        # Print AI response\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import Prompts  # Import the system prompt\n",
    "\n",
    "# Initialize Cerebras client\n",
    "api_key = os.environ.get(\"CEREBRAS_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"CEREBRAS_API_KEY environment variable is not set.\")\n",
    "\n",
    "client = Cerebras(api_key=api_key)\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add the system prompt to memory at the start\n",
    "memory.chat_memory.add_ai_message(Prompts.SYSTEM_PROMPT)\n",
    "\n",
    "# Function to interact with Cerebras API\n",
    "def get_cerebras_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": Prompts.SYSTEM_PROMPT},  # Use the imported system prompt\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=\"llama3.1-8b\",\n",
    "    )\n",
    "    # Debug: Print the entire response to inspect its structure\n",
    "    print(\"Response Object:\", response)\n",
    "    \n",
    "    # Adjust this based on the actual response structure\n",
    "    # Example: If response.choices[0].message.content is the correct path\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create a conversation chain\n",
    "def chat():\n",
    "    print(\"Welcome to the Cerebras Chat! Type '/exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to memory\n",
    "        memory.chat_memory.add_user_message(user_input)\n",
    "\n",
    "        # Get the conversation history from memory\n",
    "        history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "        # Generate a prompt with conversation history\n",
    "        prompt = f\"{history}\\nUser: {user_input}\\nAI:\"\n",
    "\n",
    "        # Get response from Cerebras API\n",
    "        ai_response = get_cerebras_response(prompt)\n",
    "\n",
    "        # Add AI response to memory\n",
    "        memory.chat_memory.add_ai_message(ai_response)\n",
    "\n",
    "        # Print AI response\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for a list of sentences\n",
    "sentences = [\"Why is fast inference important?\", \"Another example sentence.\"]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Print the embeddings\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Sentence {i+1} embedding: {embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "# =======================\n",
    "# Configuration\n",
    "# =======================\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\", \"us-west1-gcp\")\n",
    "PINECONE_INDEX_NAME = \"text-vector-store-3f9f8e1e-9d6a-4a5b-8c2d-2f1e3d4b5c6a\"  # Your specific index name\n",
    "\n",
    "CEREBRAS_API_KEY = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "CEREBRAS_MODEL = \"llama3.1-8b\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# Initialize Cerebras client\n",
    "cerebras_client = Cerebras(api_key=CEREBRAS_API_KEY)\n",
    "\n",
    "# Initialize embeddings (OpenAI or any other compatible embeddings)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load the specific Pinecone index\n",
    "if PINECONE_INDEX_NAME not in pinecone.list_indexes():\n",
    "    raise ValueError(f\"Pinecone index '{PINECONE_INDEX_NAME}' does not exist.\")\n",
    "\n",
    "vector_store = Pinecone.from_existing_index(PINECONE_INDEX_NAME, embeddings)\n",
    "\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# =======================\n",
    "# Custom LLM Wrapper for Cerebras\n",
    "# =======================\n",
    "class CerebrasLLM:\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Initialize Cerebras LLM\n",
    "cerebras_llm = CerebrasLLM(cerebras_client, CEREBRAS_MODEL)\n",
    "\n",
    "# =======================\n",
    "# RAG Pipeline with Conversation Memory\n",
    "# =======================\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cerebras_llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Chat Function\n",
    "# =======================\n",
    "def chat(query):\n",
    "    try:\n",
    "        # Get response from RAG pipeline\n",
    "        result = qa_chain({\"question\": query})\n",
    "        return result[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# =======================\n",
    "# Example Usage\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the RAG Chat! Type '/exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"/exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = chat(user_input)\n",
    "        print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:8000/create-vector-store\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"text\": \"Your text goes here. This could be any text you want to convert into a vector store.\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"overlap\": 50\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:8000/delete-vector-store\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"index_name\": \"vecstored1166eb468c7\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-d4199c81edca44709f3eb9c1d2df6f53\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello Tell me about DeepSeek\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'role', 'function_call', 'tool_calls'])\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about LeNet-5 and its architecture\" \n",
    "        }\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message.content)\n",
    "print(completion.choices[0].message.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
