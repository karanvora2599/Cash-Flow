{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from groq import Groq\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "import pinecone\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# =======================\n",
    "# Configuration\n",
    "# =======================\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "CEREBRAS_API_KEY = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "INDEX_CONFIG = {\n",
    "    \"legal-index\": {\"name\": \"legal-docs\", \"top_k\": 2},\n",
    "    \"tech-index\": {\"name\": \"technical-docs\", \"top_k\": 3}\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Client Initialization\n",
    "# =======================\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "if not CEREBRAS_API_KEY:\n",
    "    raise EnvironmentError(\"CEREBRAS_API_KEY environment variable not set\")\n",
    "\n",
    "try:\n",
    "    cerebras_client = Cerebras(api_key=CEREBRAS_API_KEY)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n",
    "# =======================\n",
    "# Multi-Index Retriever\n",
    "# =======================\n",
    "class MultiIndexRetriever(BaseRetriever):\n",
    "    def __init__(self, index_config: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.retrievers = {\n",
    "            name: Pinecone(\n",
    "                pc.Index(config[\"name\"]),\n",
    "                embeddings.embed_query,\n",
    "                \"text\"\n",
    "            ).as_retriever(search_kwargs={\"k\": config[\"top_k\"]})\n",
    "            for name, config in index_config.items()\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        combined_docs = []\n",
    "        for name, retriever in self.retrievers.items():\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source_index\"] = name  # Add source identifier\n",
    "            combined_docs.extend(docs)\n",
    "        return combined_docs\n",
    "\n",
    "# =======================\n",
    "# Groq LLM Wrapper\n",
    "# =======================\n",
    "# class GroqLLMWrapper:\n",
    "#     def __init__(self, model_name: str = \"llama3-70b-8192\"):\n",
    "#         self.client = Groq(api_key=GROQ_API_KEY)\n",
    "#         self.model = model_name\n",
    "\n",
    "#     def __call__(self, prompt: str) -> str:\n",
    "#         response = self.client.chat.completions.create(\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#             model=self.model,\n",
    "#             temperature=0.3\n",
    "#         )\n",
    "#         return response.choices[0].message.content\n",
    "\n",
    "# =======================\n",
    "# Cerebras LLM Wrapper\n",
    "# =======================\n",
    "class CerebrasLLMWrapper:\n",
    "    def __init__(self, model_name: str = \"llama3.1-8b\"):\n",
    "        self.client = Cerebras(api_key=CEREBRAS_API_KEY)\n",
    "        self.model = model_name\n",
    "\n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# =======================\n",
    "# RAG Pipeline Setup\n",
    "# =======================\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "multi_retriever = MultiIndexRetriever(INDEX_CONFIG)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=CerebrasLLMWrapper(),\n",
    "    retriever=multi_retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Chat Interface\n",
    "# =======================\n",
    "def format_response(result: Dict) -> str:\n",
    "    response = f\"Answer: {result['answer']}\\n\\nSources:\"\n",
    "    sources = {doc.metadata[\"source_index\"] for doc in result[\"source_documents\"]}\n",
    "    for source in sources:\n",
    "        response += f\"\\n- {source}\"\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Multi-Index RAG Chat (type '/exit' to quit)\")\n",
    "    while True:\n",
    "        query = input(\"\\nYou: \")\n",
    "        if query.lower() == \"/exit\":\n",
    "            break\n",
    "            \n",
    "        result = qa_chain({\"question\": query})\n",
    "        print(f\"\\nAssistant: {format_response(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Session in multiRAG chat\n",
    "!curl -X POST \"http://localhost:8000/start-session\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "           \"system_prompt\": \"You are an AI assistant that provides legal advice.\"\n",
    "         }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete session\n",
    "curl -X DELETE \"http://localhost:8000/delete-session\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "           \"session_id\": \"a1b2c3d4-e5f6-7890-abcd-1234567890ef\"\n",
    "         }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt the AI with a question\n",
    "!curl -X POST \"http://localhost:8000/chat\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "           \"session_id\": \"a1b2c3d4-e5f6-7890-abcd-1234567890ef\",\n",
    "           \"vector_stores\": [\"legal-index\", \"tech-index\"],\n",
    "           \"message\": \"What are the latest updates in data privacy laws?\",\n",
    "           \"system_prompt\": \"Please provide concise and formal responses.\"\n",
    "         }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample response from the AI\n",
    "{\n",
    "  \"session_id\": \"a1b2c3d4-e5f6-7890-abcd-1234567890ef\",\n",
    "  \"vector_stores\": [\"legal-index\", \"tech-index\"],\n",
    "  \"input\": \"What are the latest updates in data privacy laws?\",\n",
    "  \"response\": \"Answer: [AI-generated answer]\\n\\nSources:\\n- legal-index\\n- tech-index\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt the AI with a question without a systme prompt\n",
    "curl -X POST \"http://localhost:8000/chat\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "           \"session_id\": \"a1b2c3d4-e5f6-7890-abcd-1234567890ef\",\n",
    "           \"vector_stores\": [\"legal-index\", \"tech-index\"],\n",
    "           \"message\": \"How can I improve my productivity?\"\n",
    "         }'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
